@article{polyak,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris Teodorovich},
  journal={USSR Computational Mathematics and Mathematical Physics},
  year={1964}
}
@inproceedings{nag,
  title={A method of solving a convex programming problem with convergence rate $O({1}/{k^2})$},
  author={Nesterov, Yurii},
  booktitle={Soviet Mathematics Doklady},
  year={1983}
}
@inproceedings{svrg,
 author = {Johnson, Rie and Zhang, Tong},
 title = {Accelerating Stochastic Gradient Descent Using Predictive Variance Reduction},
 booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1},
 year = {2013}
} 
@inproceedings{sag,
  title={A stochastic gradient method with an exponential convergence \_rate for finite training sets},
  author={Roux, Nicolas L and Schmidt, Mark and Bach, Francis R},
  booktitle={Advances in Neural Information Processing Systems},
  year={2012}
}
@article{sdca,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  year={2013}
}
@techreport{adagrad,
    Author = {Duchi, John and Hazan, Elad and Singer, Yoram},
    Title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2010}
}
@article{adadelta,
  title={ADADELTA: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}
@article{adam,
    Author  = {Durk Kingma and Jimmy Ba},
    Title   = {Adam: A Method for Stochastic Optimization},
    Journal = {arXiv:1412.6980},
    Year    = {2014}
}
@misc{rmsprop,
  title={{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}

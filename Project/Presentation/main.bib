@article{polyak,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris Teodorovich},
  journal={USSR Computational Mathematics and Mathematical Physics},
  year={1964}
}
@inproceedings{nag,
  title={A method of solving a convex programming problem with convergence rate $O({1}/{k^2})$},
  author={Nesterov, Yurii},
  booktitle={Soviet Mathematics Doklady},
  year={1983}
}
@inproceedings{svrg,
 author = {Johnson, Rie and Zhang, Tong},
 title = {Accelerating Stochastic Gradient Descent Using Predictive Variance Reduction},
 booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1},
 year = {2013}
} 
@inproceedings{sag,
  title={A stochastic gradient method with an exponential convergence \_rate for finite training sets},
  author={Roux, Nicolas L and Schmidt, Mark and Bach, Francis R},
  booktitle={Advances in Neural Information Processing Systems},
  year={2012}
}
@article{sdca,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  year={2013}
}
@techreport{adagrad,
    Author = {Duchi, John and Hazan, Elad and Singer, Yoram},
    Title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2010}
}
@article{adadelta,
  title={ADADELTA: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}
@article{adam,
    Author  = {Durk Kingma and Jimmy Ba},
    Title   = {Adam: A Method for Stochastic Optimization},
    Journal = {arXiv:1412.6980},
    Year    = {2014}
}
@misc{rmsprop,
  title={{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}
@article{numpy,
author = {Stefan van der Walt and S. Chris Colbert and Gael Varoquaux},
title = {The NumPy Array: A Structure for Efficient Numerical Computation},
journal = {Computing in Science and Engineering},
year = {2011}
}
@article{gan,
  author    = {Ian J. Goodfellow and
               Jean Pouget{-}Abadie and
               Mehdi Mirza and
               Bing Xu and
               David Warde{-}Farley and
               Sherjil Ozair and
               Aaron C. Courville and
               Yoshua Bengio},
  title     = {Generative Adversarial Networks},
  journal   = {arXiV:1406:2661},
  year      = {2014}
}
@article{test-funcs,
  author    = {Momin Jamil and Xin-She Yang},
  title     = {A Literature Survey of Benchmark Functions For Global Optimization Problems},
  journal   = {arXiV:1308:4008},
  year      = {2013}
}
@article{hinton,
  author = {Hinton, G E and Salakhutdinov, R R},
  journal = {Science},
  title = {Reducing the dimensionality of data with neural networks},
  year = 2006
}
@article{dcgan,
  author    = {Alec Radford and
               Luke Metz and
               Soumith Chintala},
  title     = {Unsupervised Representation Learning with Deep Convolutional Generative
               Adversarial Networks},
  journal   = {arXiV:1511:06434},
  year      = {2015},
}
@article{marginaladaptivegradients,
  title={The Marginal Value of Adaptive Gradient Methods in Machine Learning},
  author={Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
  journal={arXiv preprint arXiv:1705.08292},
  year={2017}
}
